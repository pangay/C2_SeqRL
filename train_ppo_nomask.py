# é—®é¢˜ åªè€ƒè™‘äº†å•è¾¹ï¼Œæœªè€ƒè™‘å¾ªç¯
#!/usr/bin/env python3
import os
import csv
import numpy as np
from collections import defaultdict
import torch
import torch.nn as nn

from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.logger import configure

from stable_baselines3 import PPO   # âš ï¸ å¯ä»¥ç»§ç»­ä½¿ç”¨ MaskablePPOï¼Œä¹Ÿå¯æ¢ä¸ºæ™®é€š PPO
from env.env_general import SequenceRecoveryEnv


# ==========================================================
# å·¥å…·å‡½æ•°ï¼šå†²çªæ£€æµ‹ï¼ˆåŸå°ä¸åŠ¨ï¼‰
# ==========================================================
def check_conflict(sequence, substring_length, num_colors):
    L = len(sequence)
    c = num_colors
    hist_map = defaultdict(list)
    for i in range(L - substring_length + 1):
        window = sequence[i:i + substring_length]
        hist = tuple(window.count(v) for v in range(c))
        if hist in hist_map:
            return False
        hist_map[hist].append(i)
    return True


# ==========================================================
# å›è°ƒä¸æ—¥å¿—ç±»
# ==========================================================
class BestModelCallback(BaseCallback):
    def __init__(self, save_path: str, verbose=1, window_size=1000):
        super().__init__(verbose)
        self.save_path = save_path
        self.best_avg_reward = -np.inf
        self.episode_rewards = []
        self.window_size = window_size
        os.makedirs(save_path, exist_ok=True)

    def _on_step(self) -> bool:
        infos = self.locals.get("infos", [])
        for info in infos:
            if "episode" in info:
                ep_reward = float(info["episode"]["r"])
                self.episode_rewards.append(ep_reward)
                if len(self.episode_rewards) >= self.window_size:
                    avg_reward = np.mean(self.episode_rewards[-self.window_size:])
                    if avg_reward > self.best_avg_reward:
                        self.best_avg_reward = avg_reward
                        best_file = os.path.join(self.save_path, "best_maskable_ppo_model")
                        self.model.save(best_file)
                        if self.verbose > 0:
                            print(f"[BestModel] saved new best model, "
                                  f"avg_reward={avg_reward:.3f}, path={best_file}")
        return True


class RewardLoggerCallback(BaseCallback):
    """è®°å½• reward åˆ° CSV"""
    def __init__(self, log_csv_path: str, verbose=1, log_every=10):
        super().__init__(verbose)
        self.log_csv_path = log_csv_path
        self.log_every = log_every
        self.episode_rewards = []
        os.makedirs(os.path.dirname(log_csv_path) or ".", exist_ok=True)
        with open(self.log_csv_path, 'w', newline='') as f:
            csv.writer(f).writerow(["episode", "reward"])

    def _on_step(self) -> bool:
        infos = self.locals.get("infos", [])
        for info in infos:
            if "episode" in info:
                ep_reward = float(info["episode"]["r"])
                self.episode_rewards.append(ep_reward)
                ep_num = len(self.episode_rewards)
                if ep_num % self.log_every == 0:
                    self.logger.record("episode_reward", ep_reward)
                    with open(self.log_csv_path, 'a', newline='') as f:
                        csv.writer(f).writerow([ep_num, ep_reward])
        return True


# ==========================================================
# è‡ªå®šä¹‰ç‰¹å¾æå–å™¨ï¼ˆFlatten + MLPï¼‰
# ==========================================================
class FlattenExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space, features_dim=256):
        super().__init__(observation_space, features_dim)
        n_input = int(np.prod(observation_space.shape))
        self.fc = nn.Sequential(
            nn.Linear(n_input, 256),
            nn.ReLU(),
            nn.Linear(256, features_dim),
            nn.ReLU()
        )

    def forward(self, observations):
        x = observations.float().view(observations.size(0), -1)
        return self.fc(x)


# ==========================================================
# ä¸»è®­ç»ƒ
# ==========================================================
if __name__ == "__main__":
    log_path = "./ppo_logs/ppo_sequence_nomask/"
    os.makedirs(log_path, exist_ok=True)
    new_logger = configure(log_path, ["stdout", "tensorboard", "csv"])
    BC_PRETRAIN_PATH = "policy_bc_pretrained_9_27.pth"

    file_path = "train_data/test_sequences_9_27.txt"
    base_env = SequenceRecoveryEnv(
        file_path=file_path,
        substring_length=9,
        max_steps=10000,
        log_file=os.path.join(log_path, "env_debug.log"),
        enable_logging=True
    )

    # âœ… ä¸å†ä½¿ç”¨ Maskï¼Œåªä½¿ç”¨åŸå§‹ç¯å¢ƒ
    vec_env = DummyVecEnv([lambda: base_env])

    best_model_cb = BestModelCallback(save_path=log_path)
    reward_logger_cb = RewardLoggerCallback(
        log_csv_path=os.path.join(log_path, "episode_rewards.csv"),
        log_every=10
    )

    policy_kwargs = dict(
        features_extractor_class=FlattenExtractor,
        features_extractor_kwargs=dict(features_dim=256)
    )

    # âœ… MaskablePPO ä»å¯ç”¨ï¼Œä½† mask åŠŸèƒ½ä¸ä¼šå¯ç”¨ï¼›ä¹Ÿå¯æ¢ä¸º PPO
    model = PPO(
        policy="MlpPolicy",
        env=vec_env,
        learning_rate=5e-4,
        n_steps=2048,
        batch_size=128,
        n_epochs=20,
        gamma=0.99,
        verbose=1,
        tensorboard_log=log_path,
        policy_kwargs=policy_kwargs
    )

    model.set_logger(new_logger)

    checkpoint_cb = CheckpointCallback(
        save_freq=1000000,
        save_path=log_path,
        name_prefix="ppo_checkpoint"
    )

    #åŠ è½½è¡Œä¸ºå…‹éš†æƒé‡
    bc_weights = torch.load(BC_PRETRAIN_PATH)
    model.policy.mlp_extractor.load_state_dict(bc_weights, strict=False)
    print("âœ… BC æƒé‡å·²åŠ è½½åˆ° PPO ç½‘ç»œ")

    model.learn(
        total_timesteps=30000000,
        callback=[best_model_cb, reward_logger_cb, checkpoint_cb]
    )

    print("ğŸ¯ è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä¼˜æ¨¡å‹ä¿å­˜åœ¨: {os.path.join(log_path, 'best_maskable_ppo_model')}")
    print(f"Episode reward å·²ä¿å­˜åˆ° CSV: {os.path.join(log_path, 'episode_rewards.csv')}")
    print(f"ç¯å¢ƒè¿è¡Œæ—¥å¿—: {os.path.join(log_path, 'env_debug.log')}")
    print(f"TensorBoard å¯è§†åŒ–å‘½ä»¤: tensorboard --logdir {log_path}")